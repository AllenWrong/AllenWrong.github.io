### Handwritten

其他共同条件：batch_size: 64, 使用了batch_norma，latent特征是128维，project特征是32维。top1近邻

| MLP     | chead | phead | wlam | clam | 现象                                                         | 猜想                                                |
| ------- | ----- | ----- | ---- | ---- | ------------------------------------------------------------ | --------------------------------------------------- |
| 两层512 | 64    | 512   | 1    | 0.5  | 第4个epoch acc就升到了92，随着训练的进行acc发生了很大的下降。 |                                                     |
| 两层512 | 64    | 256   | 1    | 1    | acc一开始为60，但是上升缓慢，但是最优性能达到了当前最优（97.42）。 | 提高wlam的占比。在这个基础上搜搜一下参数            |
| 一层512 | 64    | 256   | 1    | 1    | 效果才到88左右                                               |                                                     |
| 三层512 | 64    | 256   | 1    | 1    | 效果才到96左右                                               |                                                     |
| 层512   | 64    | 256   | 1    | 1    |                                                              | 尝试将batch_size改成128，然后将sub_batch_size改成64 |
| 两层512 | 64    | 256   | 1    | 0.8  | batch_size为70，sub_batch_size为35，产生新高9750             | 尝试搜索一下参数clam和wlam                          |
|         |       |       |      |      |                                                              |                                                     |

### 代码变动

nn lambda相同时，cluster lambda越大效果越好



```
# compute whitening loss between nn
```

### 其他

类级别的whiten

### todo

paper code

搜索参数clam,wlam

调整wmse with nn