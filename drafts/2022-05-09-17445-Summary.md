---
title: "17445-Summary"
excerpt: ""
mathjax: true
tags: 
---

一些问题：做这样一个模型出来，如何使用最终的模型，或者如何从这个模型获取利润。

## Individual Assignment 1: Adversarial Advertisement Case Study

> 下面的总结在将来会进行修正

**Question 1: What makes the problem of detecting adversarial advertisement hard? (For example, why is a machine learning approach suiteable, rather than having a few specific hard-coded rules? Can the problem be specified well?)**

虚假广告监测任务面临数据规模巨大的问题。在大规模数据的情况下，无论是传统的机器学习还是深度学习，确保模型实时响应是非常重要的。大规模的数据计算通常是非常消耗时间的，这使得该任务必须兼顾算法的响应时间。此外，虚假广告的内容和其标签不具有简单的映射规则，它们之前的关系复杂多样，广告数据的模式非常复杂，这就使得我们无法通过硬编码的方法来检测虚假广告。与此同时，机器学习方法需要大规模的数据进行训练，并且机器学习方法善于捕捉人类很难捕捉到的高维的映射关系。这就使得使用机器学习的方法来进行虚假广告的检测成为了可能。

机器学习系统的性能需要具有高效和可靠的特点。尽管机器学习系统已经能够捕捉到数据中复杂的关系，能够在一定条件下做出合理的决策。但是，模型仍然有可能会做出不可靠的决策，在这种情况下，确保系统的可靠性也是很重要的难题。此外，在大规模数据的情况下，训练模型是非常消耗计算资源的。保证模型训练的性能也是需要解决的问题。

更具体而言，虚假广告检测面临着下面的几项问题：

- 在这个情形下，需要FP和FN指标都比较小。
- 数据的类别不平衡问题。在数据集中，大部分的数据都是正常的数据（正常的广告），而虚假的广告只占极少数。此外，虚假广告的类别多种多样，并且不同类别之间有重叠。
- 大规模的模型训练。模型必须频繁的进行训练，评估，校准和监控。
- 捕获专家知识。为了应对虚假广告的演进，
- 将专家知识用于多种并发的目的。
- 独立的评估。

**Question 2: What qualities were important to the team in building the system? Identify relevant qualities and give a brief description of how the team could check whether those qualities are sufficiently achieved (e.g., with specific measures).**

模型的性能：评估模型性能常用的指标有：acc，F1-score，recall，precision，auc等

- AUC：AUC指标的目的在于优化两个指标真阳率（TPR）和假阳率（FPR）。固定TPR，当FPR越小的时候，AUC就越大；固定FPR，当TPR越大的时候，AUC就越大。它希望TPR越大越好，希望FPR越小越好。Recall实际上是等于TPR的，优化recall就可以理解为尽可能将数据中实际的阳性样本检测出来。另外specificity等于1-FPR。因此，优化AUC实际上也是优化specificity。它的意义在于使得降低模型误报的概率。综上，AUC的目的在于优化两个指标recall和specificity，优化recall的意义在于使得模型尽可能将阳性样本检测出来，优化specificity的意义在于尽可能降低模型误报的概率。
- F1-score：f1-score指标的目的在于优化Recall和Precision这两个指标。优化Recall的意义和AUC中是一样的，都是为了让模型尽可能将实际中的阳性样本检测出来。Precision的意义是命中率，也就是实际为阳的样本在检测为阳的样本中所占的比率。优化Precision的意义在于使得模型尽可能的不漏掉有可能为阳的样本。这个目标是属于激进的。
- 当我们的场景中，误报阳性样本的成本比较大时，使用AUC就比较合适。当漏掉阳性样本成本比较大时，使用F1-score就比较合适。

模型决策可靠性：使用校准函数进行校准。校准函数可以是自定义的函数。比如，它的输入可以是决策的结果，输出是一个可靠性的指标，该指标一般是归一化的。

**Question 3: What are engineering challenges that emerged during the project and how were those addressed? Separate data analytics concerns (in building an initial model) from engineering concerns (that are important for building and running a production system) and identify at least three engineering challenges for which the team had to make decisions. Justify why these were important challenges, what potential options the team had to address them, and summarize how the team actually addressed them.**

**保持模型的大小：**学习到的模型必须足够小，以至于能够在单个机器的内存中放下。本文中主要用到了两种技术，一种是特征哈希，一种是保持模型的稀疏性。

- 特征哈希：特征哈希是在权重矩阵上做的。如果权重矩阵中有很多0的话，那么特征哈希的目的则是把权重中非零的数值映射到一个hash表上，hash表的大小实际上就控制了模型的大小。并且本文中指出：忽略冲突并没有降低性能。
- 保持模型的稀疏性：保持模型的稀疏性就是保证模型的许多权重值确实是0。本文采用了一种近似映射到L1球的方法来保持权重的稀疏性。此外，还有类似的方法，如：映射梯度的方法，在经过k步的更新后，将权重映射到一个特定半径的L1球上，但是这种方法速度有点慢；Duchi等人用了一种类似于随机中位数发现的方法来查找 $$tao$$的值，$$tao$$用于将权重映射到L1球上。本文提出的方法借鉴了这个方法。

**自动模型校准：**随着模型的训练，模型的输出的语义可能会发生变化，这就需要不断的调整决策的阈值来保证模型决策的可靠性。而手动校准这样的模型又是很不现实的，自动的模型校准在这样的系统中是必须的。本文采用的方法是使用模型的输出来学习一个校准函数，该校准函数将输出一个概率值。

**线上模型的有效自动监控：**随着系统被不断的使用，系统的输入和输出会发生很大的变化，这就需要系统能够自动监控输入和输出的变化，当输入和输出发生变化以致模型的决策结果会变得不可靠时，系统就要做出警告。本文汇总主要监控了四个层次的内容：

- 监控模型的精准度或者召回率的测试。精准度控制模型要尽可能的检测出可能为阳性的样本，使得模型宁可错杀一千不可放过一个。召回率使得模型尽可能的将实际的阳性样本检测出来。这是对模型决策可靠的基本测试。
- 监控模型输入的分布漂移。
- 监控模型输出的分布漂移。也监控模型的决策率，当模型一直不能够自动决策的时候，说明系统出现了问题，需要进行重训练。
- 监控整个系统的质量。

**向模型集成信息：**自动化的模型必须能够自动的检测虚假广告，但是在这个问题中，模型也需要知道在一些情况下怎么做。这能够降低系统的复杂度，减小管理和运维模型的负担。有意思的是，实验发现向模型集成一些有用的信息可以帮助模型去理解如何做下面的事情：

- 过滤掉可以忽略的样本。
- 对特征进行变换
- 将训练数据进行打标签，然后将测试数据从训练数据中区别出来
- 报告模型的参数
- 对一个样本进行打分
- 校准模型到一个一致的尺度上。

**并行计算的问题：**在本案例中，作者使用SGD进行优化，由于样本数量非常大，因此这样的训练过程可能非常缓慢，必须要尽可能的提高训练的速度。一般想到的方法可以是并行化这样的过程，比如采用MapReduce的方法来改造计算过程，但是SGD是一个不能并行化的算法，它必须顺序的进行计算，因此，就无法通过MapReduce的方法来改造SGD。不过，作者发现，SGD过程中耗时的原因主要在于数据的读入和变换，数据的读入和变换是可以并行化的过程，因此作者将数据的读入和变换采用MapReduce的思想进行了改造，而SGD仍然是顺序进行的。这样进一步提高训练的速度。

**Question 4: What lessons can be learned for future software projects of similar scale or importance? Identify and briefly describe at least two lessons that are worth sharing with other teams building AI-enabled systems, especially teams that are new to using AI techniques.**

- 不仅仅要监控模型的测试性能，还是要监控模型的输入和输出的分布漂移。当输入的分布发生漂移时，模型就无法基于现有的知识进行决策，这就要进行重训练。当输出的分布发生漂移时，模型的决策就变得很不可靠。
- 改进SGD的方法，在大规模数据的情况下，可以将数据的加载和变换过程进行并行化，而保持SGD的顺序计算。简单来说就是并行的做耗时的工作，串行的做快速的工作。

## Chapter 19

两种主要的评估方式：线上评估和线下评估。线上评估就是将模型部署到系统中，根据用户的反馈来评估模型的性能。线下评估就是将已有的数据划分出测试集，然后利用测试集进行评估。

**智能系统应该尽可能的准确意味着什么？**

有效的智能系统应该有以下几个特点：泛化能力强，只犯正确类型的错误，可能犯的错误。

泛化能力强是很容易理解的，因为部署的智能系统要处理的不仅仅是已经将见过的数据，还有处理未见过的数据。这就要求它不能是仅对已有数据的模式进行简单的记忆，而应该是理解已有数据中的模式，并根据这种模式对未见的数据做出决策。

- 假阳率（FPR）可以理解为检测阴性样本时犯错误的比率。
- 假阴率（FNR）可以理解为检测阳性样本时犯错误的比率。
- 真阳率（TPR）可以理解为检测的阳性样本的命中率。
- 真阴率（TNR）可以理解为检测的阴性样本的命中率

只犯正确类型的错误，就是值模型能够在混淆矩阵上达到一个合适的平衡。

一个智能系统有可能会犯人类无法理解的错误，可能导致用户的差评。因此要尽可能的保证智能系统的错误不能过于离谱。当系统犯了不可接受的错误时，系统要能够发出警报。

**如何使用数据来评估系统，以及这些评估方法的缺点**

- 评估回归输出的方法，MSE
- 评估概率输出的方法。一种是将概率输出转化成分类标签，然后计算准确度。另一种是利用对数损失。
- 评估排名系统的方法。用户的选择在topk中的占比。

测试集和训练集不能有交集。也就是说，在训练的时候不能使用测试集中的样本，否则会造成评估结果过于乐观。

对于亚群体的评估。尽管模型在总体上的性能已经能够让人满意，但是在总体的某些子群体中模型的性能却很低。这是因为这样的子群体和总体的特征有些差异，这使得模型在这样的子群体上表现不佳。因此，在评估的时候不仅仅要评估总体的性能，还要评估在这些亚群体上的性能。实际上，在评估亚群体的过程中仍然面临着两种困难，1.如何识别样本是不是属于某个亚群体。2.如何确保每个亚群体都有足够的评估样本。

对于识别样本是不是属于某个亚群体而言，可以使用手工标注，让人类手动标准上样本是否属于某个特定的亚群体。这种方式显然是费时费力费财。因此，另外的一种方法是设定一些规则，让程序自动标注样本是否属于某个亚群体，虽然这没有前者更准确，但确实是一种足够好的选择。

确保数据的数量，本书给出了一些参考：

- 成千的最新数据用于评估
- 成百的亚群体的数据用于亚群体的评估
- 剩余的数据就可以用于训练

**对比不同的模型**

ROC曲线或者PR曲线。

**主观评测**

发挥人的主观能动性，来评测一下系统的性能。比如可以从如下几个方面进行：探索一下被误分类的数据。想象用户的体验。对系统可能出现的问题进行头脑风暴。

## **TODO Concept**

**metamorphic test**