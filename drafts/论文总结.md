### An Introduction to Autoencoders [arXiv 2022]

#### 主要贡献

- 介绍了自编码器是什么，基本自编码器的结构，激活函数及其适用场景,

#### 具体内容

- 自编码器的层数通常都是奇数，隐层中神经元的个数在整个架构中最小。或者说自编码器的结构关于隐层对称。此外，隐层及其前面的层称为编码器，隐层及其后面的层称为解码器。
- 避免自编码器学习恒等函数的两个常用的技术：瓶颈和正则化
  - 瓶颈：从输入层到隐层的过程看起来像处于瓶体和瓶口中间的瓶颈。这样能够降低隐层特征的维度。瓶颈的约束能力太强或太弱都会影响自编码器的性能。一般而言，要选择合适的隐层的神经元的个数。
  - 正则化：直观上，正则化技术能够增强隐层特征的稀疏性。最常见的就是L1和L2正则化，编码器和解码器的系数捆绑。
- 激活函数主要分成两大类：ReLU和identity，Sigmoid和Softmax。当输入的值是非负数范围时，ReLU是不错的选择，如果输入的值含有负数，那么就不能使用ReLU，这时可以使用identity。但是这就使得神经网络变成了线性的。如果输入的X的范围在[0,1]之间，或者经过处理将它们标准化到了[0,1]区间内，这种情况输出层可以使用Sigmoid。
- 损失函数：常见的有MSE和BCE。
  - MSE基本上是都适用的，无关于数据是否被标准化，无关于输出层的激活函数。
  - 只有输出层的值在[0,1]范围内的时候，才可以使用BCE。比如：当输出层是Sigmoid或Softmax激活函数的时候，就可以使用BCE。
- 应用场景有：降维，分类，聚类，异常检测等。
  - 和PCA的关系：自编码器降维具有计算上的优势，当数据集的规模扩大时，自编码器计算效率往往更快。PCA对特征做的是线性变换，而自编码器做的是非线性变换。在特定情况下，自编码器和PCA是等效的[[ref](http://toe.lt/1a.)]

### Efficient Multi-Modal Fusion with Diversity Analysis [ACM MM 2021]

- 主要贡献：通过测量不同模态分支模型的性能以及不同模态分支的距离，提出了对融合模型性能的理论估计。进一步提出了模型选择框架，它能够识别能使融合模型达到最后多模态性能的候选模态分支。

### Whitening for Self-Supervised Representation Learning [ICML 2021] 

#### 主要贡献

- 

#### 概念

- 晚期融合：将每个模态的输出层进行融合
- 中期融合：将每个模态的最后一个隐藏层的线性输出（即没有激活）进行加和，然后将融合的特征送入Softmax层
- 非线性融合：是最流行的一种融合方法。非线性融合可以理解为，将每个模态的最后一个隐藏层的线性输出进行加和，然后再经过激活函数（比如ReLU）的作用，最后再经过一个线性变换（也就是经过一个线性层）然后送入Softmax层。

#### 模型结构

模型主要有两个模块：多样性估计器(diversity estimator)和性能估计器(performance estimator)。多样性估计器来估计每个分支的统计特性，而性能估计器来估计候选的融合模型的性能。

### SimCLR

本文的损失函数的形式如下：
$$
l_{i,j}=-log\frac{exp(sim(z_i,z_j)/\tau)}{\sum^{2n}_{k=1}1_{[k\ne i]}exp(sim(z_i, z_k)/\tau)}
$$
二分类的损失函数如下：
$$
l = -log(\hat{y})
$$
SimCLR中对比学习损失函数的形式可以解释正负对的二分类问题。而我们只关注正对。那么就可以认为：
$$
\hat{y}=\frac{exp(sim(z_i,z_j)/\tau)}{\sum^{2n}_{k=1}1_{[k\ne i]}exp(sim(z_i, z_k)/\tau)}
$$
如果从这个角度来看的化，这里实际上还是提供了比较强的监督信息。因为两种增强样本之间的相似度还是比较高的。甚至，我认为有这种监督信息，它有可能并不是真正意义上自监督，反而有点倾向于有监督。这么考虑的话，使用随机近邻而不是增强样本进行对比的话，那么效果肯定会差些。因为随机近邻并不能人为地去提供监督信息，只能让模型自己去计算近邻，从而去寻找正对，这里并没有人为的定义正对，因此它并没有人为地提供监督信息。

### Convex Mixture Models for Multi-view Clustering [Springer 2009]

#### 主要贡献：

- 

### What Makes for Good Views for Contrastive Learning? [NeurIPS 2020]

多视图之间的对比学习已经取得了很大的进展，在不同视图之间的选择仍然是一个重要的问题，并且研究的很少。作者认为，我们应该减少视图之间的互信息而尽可能保留任务相关的信息。并且从实验上验证了猜想。

#### 主要贡献：

- 展示了用于对比表示学习的最优的视图应该是任务相关的
- 从实验上发现了对互信息的估计值和表示的质量呈现U型的关系
- 提出了一个新的半监督方法来学习特定任务下的有效的视图

**概念**

- 充分的编码器：所谓的充分的编码器就是指

**总结**

- 它的定义出发点是不是有瑕疵
- 一系列数据增强方法。以及它们为什么能够减小互信息，源代码实现。
- 是它的数据增强方法起作用了还是它的模型学习方法起作用了。实验验证