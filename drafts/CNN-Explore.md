## AlexNet

#### 要点

- 使用ReLU激活函数。ReLu激活函数是一种非饱和的激活函数。
  - 当x趋向于正无穷时，函数的导数趋向于0，此时称为右饱和。当x趋向于负无穷时，函数的导数趋向于0，此时称为左饱和。当一个函数既左饱和又右饱和的时候，这种函数称为饱和函数如Sigmoid和tanh。
  - 非饱和激活函数可以解决梯度消失问题。饱和函数容易出现梯度消失的问题。此外，非饱和激活函数可以加快收敛。
- 使用Dropout技巧，缓解模型的过拟合
- 使用重叠最大池化技术



## TODO

ReLU的神经元死亡现象。



## Reference

https://www.zhihu.com/tardis/sogou/art/427541517



