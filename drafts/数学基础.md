## 1.梯度（Gradient）

可以将梯度看成是导数推广到了多元变量的标量函数上。所谓多元变量的标量函数即：$f:\R^n \rightarrow \R$。那么它的梯度可以表示成：
$$
\triangledown f =
\begin{bmatrix}
\dfrac{\partial f}{\partial x_1} \\
\vdots \\
\dfrac{\partial f}{\partial x_n}
\end{bmatrix}
$$
某点的梯度，指向该点速度增加最快的方向。从而$-\triangledown f$指向该点下降最快的方向。

## 2. 雅克比矩阵（Jacobian）

雅克比矩阵将导数推广到了多元变量的向量函数上。对于函数：$f:\R^n \rightarrow \R^m$，它的一阶偏导可以使用雅克比矩阵表示出来：
$$
J_f =
\begin{bmatrix}
    \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n} \\
\end{bmatrix}
$$
注意到，当$m=1$的时候，$\triangledown f=J^T_f$。

## 3. Softmax求导

### 3.1 单样本时Softmax求导

现在的深度学习方法，基本上都采用Mini Batch的训练方法。这里，先从一种简单的情况（即单样本的情况）开始讨论。假设，有一个样本为$x\in \R^k$，$k$表示类别的个数：
$$
x = 
\begin{bmatrix}
x_1 \\
\vdots \\
x_n
\end{bmatrix}

\ \ \ \ 
f(x)=softmax(x)=
\begin{bmatrix}
\frac{e^x_1}{\sum_i^k x_i} \\
\vdots \\
\frac{e^x_k}{\sum_i^k x_i}
\end{bmatrix}
$$
这时求导是向量对向量的求导。另外，Softmax的输出的维度和输入的维度是一样的。若$x \in R^n$，则$softmax(x) \in R^n$。因此Softmax​的雅克比矩阵一定是方阵。可得如下的雅克比矩阵：
$$
\frac{\partial f}{\partial x} =
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_{n}} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_k}{\partial x_1} & \cdots & \frac{\partial f_{n}}{\partial x_{n}}
\end{bmatrix}
$$
实际上，里面的每一项都是很有规律的。下面看几个特定的项：
$$
\begin{aligned}
\frac{\partial f_1}{\partial x_1} 
=& \frac{\partial}{\partial x_1}\left[\frac{e^{x_1}}{e^{x_1}+\cdots +e^{x_{n}}}\right] 
\\
=& \frac{e^{x_1}[e^{x_1}+\cdots +e^{x_{n}}]-e^{x_1}e^{x_1}}{(e^{x_1}+\cdots +e^{x_{n}})^2} \\
=& \frac{e^{x_1}[e^{x_2}+\cdots +e^{x_{n}}]}{(e^{x_1}+\cdots +e^{x_{n}})^2} \\
=& f(x_1)(1-f(x_1))
\end{aligned}
$$

无论是但从直觉的角度，还是从推理的角度，都可以验证Softmax的雅克比矩阵对角线上的元素都满足上面的公式。现在再看一下非对角线上的元素：
$$
\begin{aligned}
\frac{\partial f_1}{\partial x_2} =& \frac{\partial}{\partial x_2} 
    \left[\frac{e^{x_1}}{e^{x_1}+\cdots +e^{x_{n}}}\right] \\
=& -\frac{e^{x_1}e^{x_2}}{(e^{x_1}+\cdots +e^{x_{n}})^2} \\
=& -f(x_1)f(x_2)
\end{aligned}
$$
同样的，无论从直觉的角度，还是从推理的角度，都可以验证Softmax的雅克比矩阵非对角线上的元素都满足上面的公式。至此，我们可以将Softmax的雅克比矩阵进一步细化：
$$
\begin{aligned}
J_f=&
\begin{bmatrix}
f(x_1)(1-f(x_1)) & \cdots & -f(x_1)f(x_n) \\
-f(x_2)(f(x_1) & \cdots & -f(x_2)f(x_n) \\
\vdots & \ddots & \vdots \\
-f(x_n)(f(x_1) & \cdots & -f(x_)f(x_n) 
\end{bmatrix} \\
=&
\begin{bmatrix}
f(x_1) & & \\
 & \ddots & \\
 & & f(x_n)
\end{bmatrix} - 
\begin{bmatrix}
f(x_1) \\
f(x_2) \\
\vdots \\
f(x_n)
\end{bmatrix}
\begin{bmatrix}
f(x_1) & f(x_2) &\cdots & f(x_n)
\end{bmatrix} \\
=& np.diag(f(x)) - np.dot(f(x), f(x)^T)
\end{aligned}
$$

### 3.2 Mini Batch时Softmax求导

假设一个Mini Batch包含bs个样本。基于上面的讨论，我们可以将一个Batch的数据表示成如下的形式：
$$
x_{batch}=
\begin{bmatrix}
\vdots & \vdots & & \vdots \\
x^{\{1\}} & x^{\{2\}} & \cdots & x^{\{bs\}} \\
\vdots & \vdots & & \vdots
\end{bmatrix}_{k\times bs}
$$
在上面的表示中，$x^{\{i\}}$表示第i个样本，每个样本包含k个特征。$x^{\{i\}}_k$表示第i个样本的第k个特征。整个Batch的数据通过Softmax函数可以得到下面的结果：
$$
softmax(x_{batch})=
\begin{bmatrix}
\dfrac{e^{x^{\{1\}}_1}}{\sum_i^k x^{\{1\}}_i} & \cdots & \dfrac{e^{x^{\{bs\}}_1}}{\sum_i^k x^{\{bs\}}_i} \\
\vdots & \ddots & \vdots \\
\dfrac{e^{x^{\{1\}}_k}}{\sum_i^k x^{\{1\}}_i} & \cdots & \dfrac{e^{x^{\{bs\}}_k}}{\sum_i^k x^{\{bs\}}_i}
\end{bmatrix}
$$
基于前面单个样本的讨论，Softmax对单个样本i求导可以得到矩阵$J_f^{\{i\}}$。对这一个Batch的样本求导将会得到一堆矩阵，不妨将它表示成一个列表$[J_f^{\{1\}}, J_f^{\{2\}}, \cdots, J_f^{\{bs\}}]$。

## 范数：

范数的索引越高，范数就会越关注于较大的异常点。比如，无穷范数只关注最大的绝对值。

# 概率分布基础

从概率的角度看待机器学习，通俗的说，就是将涉及到的一切未知的量看成是**随机变量**。随机变量将会有一系列可能的取值，并且随机变量取某个值是有一个概率与之对应的。像这样，随机变量一系列可能的取值及其对应的概率就被认为是随机变量的**概率分布**。或者也可以说成是分布。在我看来，利用概率的理论去看待机器学习，对于随机变量和概率分布这样基础概念的理解是非常重要的。



**经验风险函数和损失函数的区别**

**随机向量**

**Parameter bias 不同于统计中的bias**

